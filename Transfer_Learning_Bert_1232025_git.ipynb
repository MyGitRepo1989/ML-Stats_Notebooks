{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ25iAgAOJ5K"
   },
   "source": [
    "# MMAI 894 - Exercise 3\n",
    "## Transfer learning with DistilBert\n",
    "The goal of this excercise is to build a text classifier using the pretrained DistilBert published by HuggingFace. You will be doing this using the Glue/CoLA dataset (https://nyu-mll.github.io/CoLA/).\n",
    "\n",
    "Submission instructions:\n",
    "\n",
    "- You cannot edit this notebook directly. Save a copy to your drive, and make sure to identify yourself in the title using name and student number\n",
    "- Do not insert new cells before the final one (titled \"Further exploration\") \n",
    "- Verify that your notebook can _restart and run all_. \n",
    "- Unlike previous assignments, please **submit all three formats: .py, .ipynb, and html** (see https://torbjornzetterlund.com/how-to-save-a-google-colab-notebook-as-html/)\n",
    " - The notebook and html submissions should show the completion of your best performing run\n",
    " - Submission files should be named: `studentID_lastname_firstname_ex3.py (or .html, .ipynb)`\n",
    "- The mark will be assessed on the implementation of the functions with #TODO\n",
    "- **Do not change anything outside the functions**  unless in the further exploration section\n",
    "- - As you are encouraged to explore the network configuration, 20% of the mark is based on final accuracy. \n",
    "- Note: You do not have to answer the questions in thie notebook as part of your submission. They are meant to guide you.\n",
    "\n",
    "- You should not need to use any additional libraries other than the ones listed below. You may want to import additional modules from those libraries, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hZj4c0xTeMwH"
   },
   "outputs": [],
   "source": [
    "# This cell installs and sets up DistilBert import, as well as the dataset, which we will \n",
    "# use tf.datasets to load (https://www.tensorflow.org/datasets/catalog/overview)\n",
    "\n",
    "!pip install -q transformers tfds-nightly\n",
    "\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "try: # this is only working on the 2nd try in colab :)\n",
    "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "except Exception as err: # so we catch the error and import it again\n",
    "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHvJJjnCRYF2"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q3gYLKfEd0Hb"
   },
   "outputs": [],
   "source": [
    "def load_data(save_dir=\"./\"):\n",
    "  dataset = tfds.load('glue/cola', shuffle_files=True)\n",
    "  train = tfds.as_dataframe(dataset[\"train\"])\n",
    "  val = tfds.as_dataframe(dataset[\"validation\"])\n",
    "  test = tfds.as_dataframe(dataset[\"test\"])\n",
    "  return train, val, test\n",
    "\n",
    "def prepare_raw_data(df):\n",
    "  raw_data = df.loc[:, [\"idx\", \"sentence\", \"label\"]]\n",
    "  raw_data[\"label\"] = raw_data[\"label\"].astype('category')\n",
    "  return raw_data\n",
    "\n",
    "train, val, test = load_data()\n",
    "train = prepare_raw_data(train)\n",
    "val = prepare_raw_data(val)\n",
    "test = prepare_raw_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQC3J5brmZdy"
   },
   "source": [
    "Before using this data, we need to clean and QA it. Unlike MNIST, this is a text dataset, and we should be more caerful. For example:\n",
    "- Are there any duplicate entries? \n",
    "- What is the range of lengths for the sentences? Should we impose a minimum sentence length?\n",
    "- Are there \"non-sentence\" entries? For example, hashtags or other features we should remove? (luckily, this dataset is quite clean, but that might not always be the case!)\n",
    "\n",
    "NOTE! The sentences are encoded as binary strings. To do text manipulations, you might need to decode them using `s.decode(\"utf-8\")`\n",
    "\n",
    "You may notice that that test set has no labels. This is because Glue is a benchmark dataset, and only gets scored on submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHNJC2vZmTWl",
    "outputId": "951687d3-59c1-4f8c-a7da-27aff010ffc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                                           sentence label\n",
      "0  1680  b'It is this hat that it is certain that he wa...     1\n",
      "1  1456  b'Her efficient looking up of the answer pleas...     1\n",
      "2  4223          b'Both the workers will wear carnations.'     1\n",
      "3  4093  b'John enjoyed drawing trees for his syntax ho...     1\n",
      "4  7111  b'We consider Leslie rather foolish, and Lou a...     1\n",
      "    idx                                         sentence label\n",
      "0   163            b'Brian was wiping behind the stove.'    -1\n",
      "1   131       b'You could give a headache to a Tylenol.'    -1\n",
      "2  1021                          b'I want to meet at 6.'    -1\n",
      "3   166                        b'Packages carry easily.'    -1\n",
      "4  1039  b\"Many people said they were sick who weren't.\"    -1\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "#   # TODO: What data cleaning/filtering should you consider?\n",
    "#   # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "  df['length'] = df.sentence.apply(lambda x:len(x))\n",
    "  df = df[df.length > 10]\n",
    "  df.drop(['length'],axis=1)\n",
    "  cleaned_data = df[['idx','sentence','label']]\n",
    "\n",
    "  return cleaned_data\n",
    "\n",
    "train = clean_data(train)\n",
    "val = clean_data(val)\n",
    "test = clean_data(test)\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAYstlfPQSvd"
   },
   "source": [
    "Next, we need to prepare the text for DistilBert. Instead of ingesting raw text, the model uses token IDs to map to internal embedding. Additionally, since the input is fixed size (due to our use of batches), we need to let the model know which tokens to use (i.e. are part of the sentence).\n",
    "\n",
    "Luckily, `dbert_tokenizer` takes care of all that for us - \n",
    "- Preprocessing: https://huggingface.co/transformers/preprocessing.html\n",
    "- Summary of tokenizers (DistilBert uses WordPiece): https://huggingface.co/transformers/tokenizer_summary.html#wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_RBthPA0fcTA"
   },
   "outputs": [],
   "source": [
    "def extract_text_and_y(df):\n",
    "  text = [x.decode('utf-8') for x in  df.sentence.values]\n",
    "  # for multiclass problems, you can use sklearn.preprocessing.OneHotEncoder, but we only have two classes, so we'll use a single sigmoid output\n",
    "  y = np.array([x for x in df.label.values])\n",
    "  return text, y\n",
    "\n",
    "def encode_text(text):\n",
    "    \n",
    "    inputs = dbert_tokenizer(text, padding='max_length', truncation=True, return_tensors=\"tf\", max_length = 50)\n",
    "    input_ids=inputs['input_ids']\n",
    "    attention_mask=inputs['attention_mask']\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# the following prepares the input for running in DistilBert\n",
    "train_text, train_y = extract_text_and_y(clean_data(train))\n",
    "val_text, val_y = extract_text_and_y(clean_data(val))\n",
    "test_text, test_y = extract_text_and_y(clean_data(test))\n",
    "\n",
    "train_input, train_mask = encode_text(train_text)\n",
    "val_input, val_mask = encode_text(val_text)\n",
    "test_input, test_mask = encode_text(test_text)\n",
    "\n",
    "train_model_inputs_and_masks = {\n",
    "    'inputs' : train_input,\n",
    "    'masks' : train_mask\n",
    "}\n",
    "\n",
    "val_model_inputs_and_masks = {\n",
    "    'inputs' : val_input,\n",
    "    'masks' : val_mask\n",
    "}\n",
    "\n",
    "test_model_inputs_and_masks = {\n",
    "    'inputs' : test_input,\n",
    "    'masks' : test_mask\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2USajN2MWjn"
   },
   "source": [
    "# Modelling\n",
    "\n",
    "## Build and Train Model\n",
    "\n",
    "Resources:\n",
    "- BERT paper https://arxiv.org/pdf/1810.04805.pdf\n",
    "- DistilBert paper: https://arxiv.org/abs/1910.01108\n",
    "- DistilBert Tensorflow Documentation: https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZfFboF85rIe",
    "outputId": "ad10f80b-ec4e-471c-c3f0-d6c47e9f36ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " mask_ids (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " BertModel)                     ast_hidden_state=(N               'mask_ids[0][0]']               \n",
      "                                one, 50, 768),                                                    \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 768)          0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          65664       ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 64)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,830,593\n",
      "Trainable params: 467,713\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(base_model, trainable=False, params={}):\n",
    "    # TODO: build the model, with the option to freeze the parameters in distilBERT\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    # Hint 1: the cls token (token for classification in bert / distilBert) corresponds to the first element in the sequence in DistilBert. Take a look at Figure 2 in BERT paper.\n",
    "    # Hint 2: this guide may be helpful for parameter freezing: https://keras.io/guides/transfer_learning/\n",
    "    # Hint 3: double check that your number of parameters make sense\n",
    "    # Hint 4: carefully consider your final layer activation and loss function\n",
    "\n",
    "    # Refer to https://keras.io/api/layers/core_layers/input/\n",
    "    max_seq_len = 50\n",
    "    inputs = Input(shape = (max_seq_len,), name = 'input_ids', dtype = 'int32')\n",
    "    masks  = Input(shape = (max_seq_len,), name = 'mask_ids', dtype = 'int32')\n",
    "\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    dbert_output = base_model(inputs, attention_mask=masks)\n",
    "    # dbert_last_hidden_state gets you the output encoding for each of your tokens.\n",
    "    # Each such encoding is a vector with 768 values. The first token fed into the model is [cls]\n",
    "    # which can be used to build a sentence classification network\n",
    "    dbert_last_hidden_state = dbert_output.last_hidden_state[:,0,:]\n",
    "\n",
    "\n",
    "    # Any additional layers should go here\n",
    "\n",
    "    drop_out_layer1 = Dropout(params['drop_out'], seed=params['seed'])(dbert_last_hidden_state)\n",
    "    dense_layer_1 = Dense(512, activation='relu', bias_initializer='zeros')(drop_out_layer1)\n",
    "    drop_out_layer2 = Dropout(params['drop_out'], seed=params['seed'])(dense_layer_1)\n",
    "    dense_layer_2 = Dense(128, activation='relu', bias_initializer='zeros')(drop_out_layer2)\n",
    "    dense_layer_3 = Dense(64, activation='relu', bias_initializer='zeros')(dense_layer_2)  \n",
    "    drop_out_layer4 = Dropout(params['drop_out'], seed=params['seed'])(dense_layer_3)\n",
    "\n",
    "\n",
    "    # use the 'params' as a dictionary for hyper parameter to facilitate experimentation\n",
    "\n",
    "    my_output = Dense(1, activation='sigmoid', bias_initializer='zeros')(drop_out_layer4)\n",
    "\n",
    "    #probs = Dense(??)(my_output)\n",
    "\n",
    "    model = keras.Model([inputs, masks], my_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "params={\n",
    "        'drop_out' : 0.2,\n",
    "        'seed' : 42\n",
    "        }\n",
    "\n",
    "model = build_model(dbert_model, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z3EyvQbSzu5m"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    # TODO: compile the model, include relevant auc metrics when training\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','AUC','Precision','Recall'])\n",
    "    return model\n",
    "\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nz8kT3f8zykl",
    "outputId": "c32dfb17-b070-47a7-8458-c005f77432bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 - 40s - loss: 0.3042 - accuracy: 0.6834 - auc: 0.5670 - precision: 0.7137 - recall: 0.9195 - val_loss: 0.5851 - val_accuracy: 0.6972 - val_auc: 0.6675 - val_precision: 0.6960 - val_recall: 1.0000 - 40s/epoch - 604ms/step\n",
      "Epoch 2/10\n",
      "67/67 - 30s - loss: 0.2940 - accuracy: 0.6887 - auc: 0.6350 - precision: 0.7375 - recall: 0.8666 - val_loss: 0.5700 - val_accuracy: 0.7126 - val_auc: 0.6822 - val_precision: 0.7137 - val_recall: 0.9777 - 30s/epoch - 441ms/step\n",
      "Epoch 3/10\n",
      "67/67 - 30s - loss: 0.2878 - accuracy: 0.7028 - auc: 0.6641 - precision: 0.7576 - recall: 0.8503 - val_loss: 0.5729 - val_accuracy: 0.7232 - val_auc: 0.6945 - val_precision: 0.7571 - val_recall: 0.8846 - 30s/epoch - 444ms/step\n",
      "Epoch 4/10\n",
      "67/67 - 29s - loss: 0.2816 - accuracy: 0.6996 - auc: 0.6877 - precision: 0.7638 - recall: 0.8306 - val_loss: 0.5562 - val_accuracy: 0.7387 - val_auc: 0.7015 - val_precision: 0.7409 - val_recall: 0.9583 - 29s/epoch - 438ms/step\n",
      "Epoch 5/10\n",
      "67/67 - 30s - loss: 0.2814 - accuracy: 0.7054 - auc: 0.6898 - precision: 0.7655 - recall: 0.8388 - val_loss: 0.5748 - val_accuracy: 0.7329 - val_auc: 0.7015 - val_precision: 0.7644 - val_recall: 0.8887 - 30s/epoch - 442ms/step\n",
      "Epoch 6/10\n",
      "67/67 - 30s - loss: 0.2783 - accuracy: 0.7123 - auc: 0.6991 - precision: 0.7708 - recall: 0.8421 - val_loss: 0.5550 - val_accuracy: 0.7464 - val_auc: 0.7057 - val_precision: 0.7585 - val_recall: 0.9305 - 30s/epoch - 443ms/step\n",
      "Epoch 7/10\n",
      "67/67 - 30s - loss: 0.2790 - accuracy: 0.7107 - auc: 0.6975 - precision: 0.7638 - recall: 0.8533 - val_loss: 0.5589 - val_accuracy: 0.7435 - val_auc: 0.7039 - val_precision: 0.7531 - val_recall: 0.9374 - 30s/epoch - 444ms/step\n",
      "Epoch 8/10\n",
      "67/67 - 29s - loss: 0.2772 - accuracy: 0.7135 - auc: 0.7024 - precision: 0.7733 - recall: 0.8396 - val_loss: 0.5583 - val_accuracy: 0.7232 - val_auc: 0.7092 - val_precision: 0.7177 - val_recall: 0.9903 - 29s/epoch - 437ms/step\n",
      "Epoch 9/10\n",
      "67/67 - 30s - loss: 0.2766 - accuracy: 0.7155 - auc: 0.7061 - precision: 0.7693 - recall: 0.8516 - val_loss: 0.5668 - val_accuracy: 0.7300 - val_auc: 0.7126 - val_precision: 0.7687 - val_recall: 0.8734 - 30s/epoch - 444ms/step\n",
      "Epoch 10/10\n",
      "67/67 - 29s - loss: 0.2734 - accuracy: 0.7130 - auc: 0.7148 - precision: 0.7754 - recall: 0.8344 - val_loss: 0.5453 - val_accuracy: 0.7406 - val_auc: 0.7150 - val_precision: 0.7506 - val_recall: 0.9374 - 29s/epoch - 440ms/step\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, model_inputs_and_masks_train, model_inputs_and_masks_val,\n",
    "    y_train, y_val, batch_size, num_epochs):\n",
    "    # TODO: train the model\n",
    "    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION\n",
    "    \n",
    "    tensorflow.random.set_seed(211)\n",
    "\n",
    "    class_weight = {0 : 0.6,\n",
    "                    1 : 0.4}\n",
    "\n",
    "    history = model.fit(\n",
    "        x = [model_inputs_and_masks_train['inputs'], model_inputs_and_masks_train['masks']],\n",
    "        y = y_train,\n",
    "        epochs = num_epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_data = ([model_inputs_and_masks_val['inputs'], model_inputs_and_masks_val['masks']], y_val),\n",
    "        verbose=2,\n",
    "        class_weight = class_weight\n",
    "    )\n",
    "\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "model, history = train_model(model, train_model_inputs_and_masks, val_model_inputs_and_masks, train_y, val_y, batch_size=128, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xid8Xf2naNZW"
   },
   "source": [
    "# Further exploration (REMOVE ALL CODE AFTER THIS CELL BEFORE SUBMISSION)\n",
    "Any code after this is not evaluated, and must be removed before submission.\n",
    "Leaving code below will result in losing marks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20217864_maalouf_roy_ex3.py",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
