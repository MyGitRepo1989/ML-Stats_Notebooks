{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bergum/xtremedistil-l6-h384-go-emotion\")\n",
    "model_emotion = AutoModelForSequenceClassification.from_pretrained(\"bergum/xtremedistil-l6-h384-go-emotion\")\n",
    "\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', \\\n",
    "                      return_all_scores=True)\n",
    "\n",
    "classifier_emotion = pipeline(\"text-classification\", model=model_emotion, tokenizer=tokenizer,return_all_scores=True)\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_dict = {word: 1 for word in stopwords.words(\"english\")}\n",
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "print(tweepy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP TWEEPY\n",
    "\n",
    "CONSUMER_KEY ='Yourkey'\n",
    "CONSUMER_SECRET='Yourkey'\n",
    "ACCESS_TOKEN='Yourkey'\n",
    "ACCESS_TOKEN_SECRET='Yourkey'\n",
    "\n",
    "BT='YourkeyYourkey'\n",
    "\n",
    "client2 = tweepy.Client(bearer_token=BT)\n",
    "\n",
    "client = tweepy.Client(consumer_key=CONSUMER_KEY,\n",
    "                       consumer_secret=CONSUMER_SECRET,\n",
    "                       access_token=ACCESS_TOKEN,\n",
    "                       access_token_secret=ACCESS_TOKEN_SECRET,\n",
    "                      bearer_token=BT)\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "\n",
    "#SETUP AUTHORIZATION OF ANOTHER TWITTER AC\n",
    "\n",
    "consumer_key = CONSUMER_KEY\n",
    "consumer_secret = CONSUMER_SECRET\n",
    "callback_uri = 'oob'\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "   consumer_key, consumer_secret,\n",
    "   ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    ")\n",
    "\n",
    "oauth1_user_handler = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret,\n",
    "    callback=callback_uri\n",
    ")\n",
    "\n",
    "print(oauth1_user_handler.get_authorization_url(signin_with_twitter=True))\n",
    "\n",
    "token =input(\"please enter tokenfrom this url\")\n",
    "#token=6580548\n",
    "\n",
    "access_token, access_token_secret = oauth1_user_handler.get_access_token(token)\n",
    "\n",
    "tweet_client = tweepy.Client(\n",
    "    consumer_key=consumer_key,\n",
    "    consumer_secret=consumer_secret,\n",
    "    access_token=access_token, \n",
    "    access_token_secret=access_token_secret\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SETUP SEARCH WORDS\n",
    "searchlist=[\"i miss you today\", \"worst feeling ever\", \"this is not ok\", \"not cool\",\\\n",
    "\"heavy heart\",\"im so mad\",\"like i failed\" , \"i give up\" , \" makes me crazy\"\\\n",
    "\"start over again\",\"sad today\", \"just lonely\", \\\n",
    "\"feel like shit\",\"so bummed\" ,\" i dont care\" ,\" go to hell\"]\n",
    "\n",
    "\n",
    "messages= [\"ü§ó‚ù§Ô∏è ((hugs)) ‚òïÔ∏èüç™ \",\"HUGS!! from me ü§óüòäüç™‚ù§Ô∏è\" ,\"((hugs)) ü•∞üòç‚òïÔ∏èüç™\",\"‚ô•Ô∏èüôèüèΩ\", \"Sending you love ü§ó‚ô•Ô∏è‚òïÔ∏èüç™\", \"Many ((hugs)) from me  ü§ó‚ô•Ô∏è‚òïÔ∏èüç™\" , \"‚ô•Ô∏è\",\"ü§ó‚ô•Ô∏è\",\"‚ô•Ô∏è u\" , \"lots of love ‚ô•Ô∏è \",           \"ü§ó‚ù§Ô∏è No Sad, Happy Happy ü§ó‚ô•Ô∏è\", \"‚ô•Ô∏è\", \"No Sad üç™‚ù§Ô∏è\", \"üòä no sad ‚òïÔ∏è\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 30\n",
      "found 30\n",
      "found 30\n",
      "found 29\n",
      "found 29\n",
      "found 29\n",
      "found 30\n",
      "found 30\n",
      "found 0\n",
      "found 29\n",
      "found 28\n",
      "found 29\n",
      "found 29\n",
      "found 30\n",
      "found 30\n",
      "found tweets (132, 9)\n",
      "GETTING REPLIES METRICS _ PUBLIC METRICS\n",
      "FOUND METRICS (132, 4)\n",
      "FILTERED TO NOT VERY POPULAR (132, 13) (89, 13)\n",
      "THIS IS THE TWEET DB I WILL USE\n",
      "   index  retweet  reply  like  quote              tweetid  \\\n",
      "0     20        0      0     0      0  1594371389582372864   \n",
      "1     98        0      0     0      0  1594371237907668992   \n",
      "2    113        0      1     0      0  1594371422633205762   \n",
      "\n",
      "                                          tweet_text              user_id  \\\n",
      "0  @CharlotteKhuma3 @HiIamNicholas @piersmorgan N...            271100615   \n",
      "1  Chris caption just made me even more upset. Li...  1299856704696389633   \n",
      "2  also want to clarify i dont care if you ship t...  1517604124388474888   \n",
      "\n",
      "  username       screenname                      description  friends_count  \\\n",
      "0        F  Sincerely_farah  I will criticise you. üá∞üáº305  ‚ôçÔ∏è            198   \n",
      "1      D‚ù§Ô∏è   __theerealestt       #longlivemyangelsü´∂üèΩüïäüíî 27‚ôëÔ∏è            381   \n",
      "2      ash     echoed_beatz       vivid gay squad enthusiast            168   \n",
      "\n",
      "   follower_count                       location  \n",
      "0             175                        Kuwait   \n",
      "1             370  UN my business? Dont do thatüìç  \n",
      "2             148                she/they, minor  \n"
     ]
    }
   ],
   "source": [
    "tweetid_l, tweet_text_l, userid_l, screenname_l, username_l, location_l, description_l, followers_count_l,friends_count_l, created_at_l, website_l= [],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "for s in range(len(searchlist)):\n",
    "    response= client.search_recent_tweets(query = searchlist[s] ,max_results=30,expansions = [\"author_id\"],user_fields=['profile_image_url'])\n",
    "    results= response[-1]['result_count']\n",
    "    print(\"found\", results)\n",
    "    for i in range(results-20):\n",
    "        tweetid_l.append(response[0][i][\"id\"])\n",
    "\n",
    "        tweet_text_l.append(response[0][i][\"text\"])\n",
    "\n",
    "        #print(i)\n",
    "        userid_l.append(response[1][\"users\"][i][\"id\"])\n",
    "\n",
    "        username_l.append(response[1][\"users\"][i][\"name\"])\n",
    "\n",
    "        user_info =api.lookup_users( screen_name=None, user_id=[response[-3][\"users\"][i][\"id\"]])\n",
    "\n",
    "        #print(user_info)                                                     \n",
    "        screenname_l.append(user_info[0]._json[\"screen_name\"])                                                    \n",
    "        description_l.append(user_info[0]._json[\"description\"] ) \n",
    "        friends_count_l.append(user_info[0]._json[\"friends_count\"] ) \n",
    "        followers_count_l.append(user_info[0]._json[\"followers_count\"])\n",
    "        location_l.append(user_info[0]._json[\"location\"])\n",
    "\n",
    "\n",
    "\n",
    "leads_bd= pd.concat([ pd.Series(tweetid_l),pd.Series(tweet_text_l),pd.Series(userid_l),pd.Series(username_l),pd.Series(screenname_l), pd.Series(description_l),            pd.Series(friends_count_l),            pd.Series(followers_count_l),            pd.Series(location_l) ],axis=1)\n",
    "\n",
    "leads_bd.columns=[\"tweetid\",\"tweet_text\",'user_id','username','screenname','description', 'friends_count','follower_count','location']\n",
    "\n",
    "\n",
    "\n",
    "print(\"found tweets\", leads_bd.shape )\n",
    "print(\"GETTING REPLIES METRICS _ PUBLIC METRICS\")\n",
    "\n",
    "retweet,reply,like,quote =[],[],[],[]\n",
    "\n",
    "for i in range(leads_bd.shape[0]):\n",
    "    time.sleep(random.sample([2,1],k=1)[0])\n",
    "    response  = client.get_tweets(ids=leads_bd[\"tweetid\"][i],tweet_fields=[\"public_metrics\"], expansions=[\"attachments.media_keys\"],media_fields=[\"public_metrics\"])\n",
    "    try:\n",
    "        retweet.append(response.data[0].public_metrics['retweet_count'])\n",
    "        reply.append(response.data[0].public_metrics['reply_count'])\n",
    "        like.append(response.data[0].public_metrics['like_count'])\n",
    "        quote.append(response.data[0].public_metrics['quote_count'])\n",
    "    except Exception:\n",
    "        retweet.append(0)\n",
    "        reply.append(0)\n",
    "        like.append(0)\n",
    "        quote.append(0)\n",
    "\n",
    "\n",
    "metrics_db=pd.concat([pd.Series(retweet),pd.Series(reply),pd.Series(like),pd.Series(quote)],axis=1)\n",
    "metrics_db.columns=[\"retweet\",\"reply\",\"like\",\"quote\"]\n",
    "print(\"FOUND METRICS\", metrics_db.shape)\n",
    "all_searches= pd.concat([metrics_db,leads_bd],axis=1)\n",
    "\n",
    "#MIX UP THE DB\n",
    "all_searches_v1_f=all_searches.sample(frac=1)\n",
    "\n",
    "#reducing the high volume tweets\n",
    "all_searches_v1_f2 = all_searches_v1_f.loc[all_searches_v1_f['retweet']<=10]\n",
    "all_searches_v2 = all_searches_v1_f2.loc[all_searches_v1_f2['like']<=10]\n",
    "all_searches_v3 = all_searches_v2.loc[all_searches_v2['reply']<=10]\n",
    "\n",
    "print(\"FILTERED TO NOT VERY POPULAR\",all_searches.shape,all_searches_v3.shape)\n",
    "\n",
    "\n",
    "#RESET INDEX\n",
    "all_searches_v3.reset_index(inplace=True)\n",
    "print(\"THIS IS THE TWEET DB I WILL USE\")\n",
    "print(all_searches_v3.head(3))\n",
    "\n",
    "\n",
    "\n",
    "#MAKE TWEET IDS\n",
    "topic_text= all_searches_v3[\"tweet_text\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Clean Tweets\n",
    "topic_clean=[]\n",
    "\n",
    "for i in range(len(topic_text)):\n",
    "    #print(topic_text[i].text,i)\n",
    "\n",
    "    s = topic_text[i]\n",
    "    s = \" \".join([word for word in s.split() if word not in stopwords_dict])\n",
    "    s =re.sub('RT', '', s)\n",
    "    s =re.sub('https://\\w+.\\w+/\\w+', '', s)\n",
    "    s =re.sub('\\@\\w+','', s)\n",
    "\n",
    "    s =re.sub('\\#\\w+', '', s)\n",
    "    s=re.sub('\\n', '', s)\n",
    "    s =emoji.replace_emoji(s)\n",
    "    s =re.sub('[^A-Za-z0-9]+', ' ', s)\n",
    "    s =re.sub('\\n+ ', '', s)\n",
    "    topic_clean.append(s)\n",
    "\n",
    "\n",
    "\n",
    "messages1= [\"ü§ó‚ù§Ô∏è ((hugs)) ‚òïÔ∏èüç™ \",\"HUGS!! from me ü§óüòäüç™‚ù§Ô∏è\" ,\"((hugs)) ü•∞üòç‚òïÔ∏èüç™\",\"‚ô•Ô∏èüôèüèΩ\", \"Sending you love ü§ó‚ô•Ô∏è‚òïÔ∏èüç™\",\\\n",
    "          \"Many ((hugs)) from me  ü§ó‚ô•Ô∏è‚òïÔ∏èüç™\" , \"‚ô•Ô∏è\",\"ü§ó‚ô•Ô∏è\",\"‚ô•Ô∏è u\" , \"lots of love ‚ô•Ô∏è \", \\\n",
    "          \"ü§ó‚ù§Ô∏è No Sad, Happy Happy ü§ó‚ô•Ô∏è\", \"‚ô•Ô∏è\", \"No Sad üç™‚ù§Ô∏è\", \"üòä no sad ‚òïÔ∏è\"\n",
    "          ]\n",
    "\n",
    "messages2= [\"Just Relax üç¨‚òïÔ∏èüç™ \",\"Sending you smiles üç¨‚òïÔ∏èüç™ \", \"It's going to be ok üß∏\" ,\"üòäRelax üòäüç¨\"]\n",
    "\n",
    "messages3= [\"Sending sunshine ü§ó‚ô•Ô∏è\", \"üç¨I understand üß∏ \", \"Your so ü§óüòä Coolüß∏\", \" \"]\n",
    "\n",
    "\n",
    "\n",
    "hug1=\"hug1.gif\"\n",
    "hug2=\"hug2.gif\"\n",
    "hug3=\"hug3.gif\"\n",
    "hug4=\"hug4.gif\"\n",
    "hug5=\"hug5.gif\"\n",
    "hug6=\"hug6.gif\"\n",
    "hug7=\"hug7.gif\"\n",
    "\n",
    "happy1=\"happy1.gif\"\n",
    "happy2=\"happy2.gif\"\n",
    "\n",
    "angry1=\"angry1.gif\"\n",
    "angry2=\"angry2.gif\"\n",
    "\n",
    "\n",
    "\n",
    "#classify emotions _12 _BIG LIST\n",
    "emotion_nothing=['neutral']\n",
    "emotion_funny=[\"amusement\"]\n",
    "emotion_thanks=[\"relief\",'gratitude']\n",
    "emotion_blush=[\"embarrassment\",]\n",
    "emotion_love=[\"love\",'desire',\"caring\",'admiration']\n",
    "emotion_fear=[\"fear\",\"nervousness\",]\n",
    "emotion_happy=[\"optimism\",'approval','pride',\"excitement\"]\n",
    "emotion_confused=[\"suprise\",\"confusion\"]\n",
    "emotion_angry=[\"anger \",\"disgust\",\"annoyance\",\"disapproval\"]\n",
    "emotion_sad=[\"sadness\",'disappointment']\n",
    "emotin_greif =[\"greif\",\"remorse\"]\n",
    "emotin_notlisted =[\"notlisted\",\"na\"]\n",
    "emotion_list_12 =[emotin_greif,emotion_sad,emotion_angry,emotion_confused,emotion_happy,emotion_fear,emotion_love,\\\n",
    "              emotion_blush,emotion_thanks,emotion_funny,emotion_nothing,emotin_notlisted]\n",
    "emotion_list_name =[\"emotin_greif\",\"emotion_sad\",\"emotion_angry\",\"emotion_confused\",\\\n",
    "                    \"emotion_happy\",\"emotion_fear\",\"emotion_love\",\"emotion_blush\",\"emotion_thanks\",\"emotion_funny\",\"emotion_nothing\",\"emotin_notlisted\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "message_greif =[\"I'm here with you today ‚ô•Ô∏è\",\\\n",
    "                \"sorry ‚òπÔ∏è ‚ô•Ô∏èüß∏\",\\\n",
    "                \"Stay strong ‚ô•Ô∏è and wishing you better days\"]\n",
    "\n",
    "message_sad= [\"going for a nice walk sometimes helps cheer me up üòä‚ô•Ô∏è\",\\\n",
    "              \"Stay strong ‚ô•Ô∏è\",\\\n",
    "              \"üåûand sunshine is just right the corner , summer 2023 \" ,\\\n",
    "              \"good things take time\" ,]\n",
    "\n",
    "message_angry= [\"üç™ relax  and just calm down\",\"just breatheüòÖ\",\\\n",
    "                \" just relax üç¨ \"  ,\"good things take time\"  ]\n",
    "\n",
    "message_confused= [\"flip a coin üòÇ\",\"wondering as well\",\"a mystery üòÖ\" ,\"idk üòä‚ô•Ô∏è\" ,\"confusing üòä\"]\n",
    "\n",
    "message_happy= [\"what a beautiful dayüåû \", \" positiveüåû\" , \"keep smiling ‚ò∫Ô∏èüòä\" ,]\n",
    "\n",
    "message_fear= [\"im not scared\", \" So not afraid\", \"your stronger than this, no fear\"]\n",
    "\n",
    "message_love= [\"üòä‚ô•Ô∏è\",\"‚ô•Ô∏è\",\"love is precious\",\"so precious\"]\n",
    "\n",
    "message_blush= [\"you look good today \",\"hello cutie\", \"your very good loking\", \"i like you too\", \"hello ‚ò∫Ô∏èüòä\"]\n",
    "\n",
    "message_thanks= [\"what a beautiful dayüåû \", \" positiveüåû\" , \"keep smiling ‚ò∫Ô∏èüòä\" ,]\n",
    "\n",
    "message_funny= [\"üòÇ\",\"strangely funnyüòÇ\",\"lol üòÇ\" ,\"a little amusingüòÇüòÇ\",\"üòÇüòÖ\"]\n",
    "\n",
    "message_nothing= [\"üç™‚òïÔ∏èüç¨\",\"üç™\",\"‚òïÔ∏è\",\"‚ô•Ô∏è\",\"c((hugs))\",\"‚ô•Ô∏è‚òïÔ∏è for you\"]\n",
    "\n",
    "message_notlisted= [\"‚ô•Ô∏è\",\"‚ô•Ô∏è‚òïÔ∏è\"]\n",
    "\n",
    "\n",
    "message_list=[message_greif,message_sad,message_angry,message_confused,\\\n",
    "                    message_happy,message_fear,message_love,\\\n",
    "              message_blush,message_thanks,message_funny,message_nothing,emotin_notlisted]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emotin_greif',\n",
       " 'emotion_sad',\n",
       " 'emotion_angry',\n",
       " 'emotion_confused',\n",
       " 'emotion_happy',\n",
       " 'emotion_fear',\n",
       " 'emotion_love',\n",
       " 'emotion_blush',\n",
       " 'emotion_thanks',\n",
       " 'emotion_funny',\n",
       " 'emotion_nothing',\n",
       " 'emotin_notlisted']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2\n",
      "10\n",
      "2\n",
      "9\n",
      "1\n",
      "10\n",
      "8\n",
      "6\n",
      "10\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "2\n",
      "10\n",
      "10\n",
      "10\n",
      "4\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "2\n",
      "10\n",
      "10\n",
      "2\n",
      "10\n",
      "2\n",
      "10\n",
      "10\n",
      "2\n",
      "10\n",
      "10\n",
      "6\n",
      "10\n",
      "10\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "10\n",
      "10\n",
      "6\n",
      "2\n",
      "2\n",
      "12\n",
      "1\n",
      "6\n",
      "10\n",
      "1\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "1\n",
      "10\n",
      "10\n",
      "10\n",
      "2\n",
      "10\n",
      "9\n",
      "10\n",
      "1\n",
      "8\n",
      "6\n",
      "6\n",
      "2\n",
      "10\n",
      "10\n",
      "2\n",
      "1\n",
      "10\n",
      "10\n",
      "10\n",
      "2\n",
      "2\n",
      "9\n",
      "6\n",
      "2\n",
      "10\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "emotion_list=[]\n",
    "media_list=[]\n",
    "message_to_post=[]\n",
    "\n",
    "for e in topic_clean:\n",
    "    prediction = classifier_emotion(e)\n",
    "    emotion= pd.DataFrame(prediction[0]).sort_values(by=\"score\",ascending=False)[:1][\"label\"].values[0].split(\" \")[0]\n",
    "    emotion_list.append(emotion)\n",
    "\n",
    "\n",
    "    d=\"\"\n",
    "    i=0\n",
    "    for d in emotion_list_12:\n",
    "        #print(emotion_list[i])\n",
    "        if emotion in d:\n",
    "            detected_list = d\n",
    "            message_to_post.append((random.sample(message_list[i],k=1))[0])\n",
    "            #print(detected_list)\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    media_list.append(random.sample([hug1,hug2,hug3],k=1))\n",
    "#print(message_to_post)\n",
    "#print(emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'annoyance'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prediction[0]).sort_values(by=\"score\",ascending=False)[:1][\"label\"].values[0].split(\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_list_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emotin_greif',\n",
       " 'emotion_sad',\n",
       " 'emotion_angry',\n",
       " 'emotion_confused',\n",
       " 'emotion_happy',\n",
       " 'emotion_fear',\n",
       " 'emotion_love',\n",
       " 'emotion_blush',\n",
       " 'emotion_thanks',\n",
       " 'emotion_funny',\n",
       " 'emotion_nothing',\n",
       " 'emotin_notlisted']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detected_list=[]\n",
    "i=0\n",
    "for d in emotion_list_12:\n",
    "    #print(emotion_list[i])\n",
    "    if emotion in d:\n",
    "        detected_list.append(emotion_list_name[i])\n",
    "        break\n",
    "    m_index=i\n",
    "    i+=1\n",
    "if len(detected_list) == 0:\n",
    "    detected_list.append(\"notlisted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notlisted'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(message_list[11],k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['üòÇ', 'strangely funnyüòÇ', 'lol üòÇ', 'a little amusingüòÇüòÇ', 'üòÇüòÖ']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Not necessarily grandchildren may also look d...</td>\n",
       "      <td>üç™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annoyance</td>\n",
       "      <td>Chris caption made even upset Like man could v...</td>\n",
       "      <td>just relax üç¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>also want clarify dont care ship wont hate twe...</td>\n",
       "      <td>üç™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annoyance</td>\n",
       "      <td>gala kharab hona worst feeling ever</td>\n",
       "      <td>just relax üç¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amusement</td>\n",
       "      <td>takemob tried colour like pro failed im laugh</td>\n",
       "      <td>a little amusingüòÇüòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>amusement</td>\n",
       "      <td>finishing midnight fight express need dlc need...</td>\n",
       "      <td>‚ô•Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>admiration</td>\n",
       "      <td>You cool better perk bitch</td>\n",
       "      <td>good things take time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>annoyance</td>\n",
       "      <td>don t feel like i ll never get certain shit</td>\n",
       "      <td>‚òïÔ∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>neutral</td>\n",
       "      <td>November 26 dont care really</td>\n",
       "      <td>just relax üç¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>annoyance</td>\n",
       "      <td>i m hyperbolic worst fg i ve ever played feel...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                                                  1  \\\n",
       "0      neutral   Not necessarily grandchildren may also look d...   \n",
       "1    annoyance  Chris caption made even upset Like man could v...   \n",
       "2      neutral  also want clarify dont care ship wont hate twe...   \n",
       "3    annoyance               gala kharab hona worst feeling ever    \n",
       "4    amusement     takemob tried colour like pro failed im laugh    \n",
       "..         ...                                                ...   \n",
       "84   amusement  finishing midnight fight express need dlc need...   \n",
       "85  admiration                        You cool better perk bitch    \n",
       "86   annoyance        don t feel like i ll never get certain shit   \n",
       "87     neutral                      November 26 dont care really    \n",
       "88   annoyance   i m hyperbolic worst fg i ve ever played feel...   \n",
       "\n",
       "                        2  \n",
       "0                       üç™  \n",
       "1           just relax üç¨   \n",
       "2                       üç™  \n",
       "3           just relax üç¨   \n",
       "4      a little amusingüòÇüòÇ  \n",
       "..                    ...  \n",
       "84                     ‚ô•Ô∏è  \n",
       "85  good things take time  \n",
       "86                     ‚òïÔ∏è  \n",
       "87          just relax üç¨   \n",
       "88                    NaN  \n",
       "\n",
       "[89 rows x 3 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.Series(emotion_list),pd.Series(topic_clean),pd.Series(message_to_post),],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good things take time',\n",
       " 'a little amusingüòÇüòÇ',\n",
       " 'what a beautiful dayüåû ',\n",
       " 'lol üòÇ',\n",
       " 'sorry ‚òπÔ∏è ‚ô•Ô∏èüß∏',\n",
       " 'lol üòÇ',\n",
       " 'keep smiling ‚ò∫Ô∏èüòä',\n",
       " 'strangely funnyüòÇ',\n",
       " 'good things take time',\n",
       " 'Stay strong ‚ô•Ô∏è and wishing you better days',\n",
       " 'a little amusingüòÇüòÇ',\n",
       " 'strangely funnyüòÇ',\n",
       " 'sorry ‚òπÔ∏è ‚ô•Ô∏èüß∏',\n",
       " \"I'm here with you today ‚ô•Ô∏è\",\n",
       " 'lol üòÇ',\n",
       " \"I'm here with you today ‚ô•Ô∏è\",\n",
       " 'your stronger than this, no fear',\n",
       " 'a little amusingüòÇüòÇ',\n",
       " 'going for a nice walk sometimes helps cheer me up üòä‚ô•Ô∏è',\n",
       " 'Stay strong ‚ô•Ô∏è and wishing you better days',\n",
       " 'sorry ‚òπÔ∏è ‚ô•Ô∏èüß∏',\n",
       " 'lol üòÇ',\n",
       " 'sorry ‚òπÔ∏è ‚ô•Ô∏èüß∏',\n",
       " 'strangely funnyüòÇ',\n",
       " 'im not scared',\n",
       " 'sorry ‚òπÔ∏è ‚ô•Ô∏èüß∏',\n",
       " 'üòÇüòÖ',\n",
       " 'good things take time',\n",
       " 'Stay strong ‚ô•Ô∏è and wishing you better days',\n",
       " 'üòÇ',\n",
       " 'going for a nice walk sometimes helps cheer me up üòä‚ô•Ô∏è']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_to_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MAKE TWEET IDS\n",
    "tweet_ids= all_searches_v3[\"tweetid\"].values\n",
    "\n",
    "#MediAPI\n",
    "media_api = tweepy.API(oauth1_user_handler )\n",
    "\n",
    "\n",
    "#NOW WE WILL SEND HUGS REPLIES\n",
    "\n",
    "\n",
    "for i in range(len(tweet_ids)):\n",
    "    try:\n",
    "        e_reply= message_to_post[i]\n",
    "        #reply = random.sample(messages,k=1)[0] \n",
    "        #print(all_searches_v3[\"tweet_text\"].loc[i])  \n",
    "\n",
    "        print(tweet_ids[i])\n",
    "        print(e_reply)\n",
    "        #print(\"HUGGED SOMEONE\", i,len(tweet_ids))\n",
    "\n",
    "       \n",
    "        print(\"I REPLIED\", i,len(tweet_ids))\n",
    "        time.sleep(random.sample([11,18,9,13],k=1)[0])\n",
    "\n",
    "\n",
    "        if media_list[i]==\"skip\":\n",
    "          \n",
    "\n",
    "        if emotion_list[i]==\"anger\"|\"annoyance\"|sadness:\n",
    "\n",
    "            print(\"GIPHY\",emotion_list[i])\n",
    "\n",
    "            media = media_api.media_upload(angry1).media_id\n",
    "            print(media)\n",
    "\n",
    "\n",
    "\n",
    "        if random.sample([297,13,0,297,15,49],k=1)[0] ==297:\n",
    "            tweet_client.like(tweet_id=tweet_ids[i])\n",
    "        print(all_searches_v3[\"tweet_text\"][i])\n",
    "\n",
    "        time.sleep(random.sample([11,18,9,49,13],k=1)[0])\n",
    "\n",
    "    except Exception:\n",
    "        print(\"ERROR\")\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "print(\"SET ONE DONE\")\n",
    "print (z)\n",
    "time.sleep(297)\n",
    "time.sleep(297)\n",
    "time.sleep(297)\n",
    "\n",
    "\n",
    "'''\n",
    "#classify emotions\n",
    "emotion_list=[]\n",
    "media_list=[]\n",
    "message_to_post=[]\n",
    "\n",
    "for e in topic_clean:\n",
    "    prediction = classifier(e)\n",
    "    emotion= pd.DataFrame(prediction[0]).sort_values(by=\"score\",ascending=False).reset_index().loc[0][\"label\"]\n",
    "    emotion_list.append(emotion)\n",
    "\n",
    "    if emotion == \"joy\":\n",
    "        m_to_post=random.sample([happy1,happy2],k=1)[0]\n",
    "        message=random.sample(messages3,k=1)[0]\n",
    "    elif emotion == \"anger\":\n",
    "        m_to_post=random.sample([angry1,angry2],k=1)[0]\n",
    "        message=random.sample(messages2,k=1)[0]\n",
    "    elif emotion == \"love\":\n",
    "        m_to_post=random.sample([hug1,hug2,hug3,hug4,hug5],k=1)[0]\n",
    "        message=random.sample(messages1,k=1)[0]\n",
    "\n",
    "    else:\n",
    "        message=random.sample(messages1,k=1)[0]\n",
    "        m_to_post=\"skip\"\n",
    "\n",
    "    message_to_post.append(message)    \n",
    "    media_list.append(m_to_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25,len(tweet_ids)):\n",
    "    e_reply= message_to_post[i-7]\n",
    "    #reply = random.sample(messages,k=1)[0] \n",
    "    #print(all_searches_v3[\"tweet_text\"].loc[i])  \n",
    "\n",
    "    print(tweet_ids[i])\n",
    "    print(e_reply)\n",
    "    #print(\"HUGGED SOMEONE\", i,len(tweet_ids))\n",
    "\n",
    "    tweet_client.create_tweet(in_reply_to_tweet_id=tweet_ids[i],text=e_reply)\n",
    "    print(\"I REPLIED\", i,len(tweet_ids))\n",
    "    time.sleep(random.sample([11,18,9,13],k=1)[0])\n",
    "\n",
    "\n",
    "    if media_list[i]==\"skip\":\n",
    "        tweet_client.create_tweet(in_reply_to_tweet_id=tweet_ids[i],text=e_reply)\n",
    "\n",
    "    if emotion_list[i] in [\"anger\",\"annoyance\",\"sadness\"]:\n",
    "        print(\"GIPHY\",emotion_list[i])\n",
    "        media = media_api.media_upload(angry1).media_id\n",
    "        print(media)\n",
    "        tweet_client.create_tweet(in_reply_to_tweet_id=tweet_ids[i],media_ids= [media],text=e_reply)\n",
    "        \n",
    "\n",
    "    if random.sample([297,13,0,297,15,49],k=1)[0] ==297:\n",
    "        tweet_client.like(tweet_id=tweet_ids[i])\n",
    "    print(all_searches_v3[\"tweet_text\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
